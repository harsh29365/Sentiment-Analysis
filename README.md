Dataset: https://www.kaggle.com/datasets/andrewmvd/steam-reviews 

Text Embeddings: 
It is the process of converting text to a fixed length array of numbers. These numbers have 
the meaning of the text encoded in them. We can apply various mathematical operations on 
these numbers to analyse the text. It is an important preprocessing step in natural language 
machine learning tasks. 

all-mpnet-base-v2: 
It is an advanced text to vector model that converts text to a 1D vector of 768 numbers. It is 
meant to be used as a short paragraph encoder. Given an input text, it returns a vector which 
captures the semantic information. It can be used to generate high quality embeddings for our 
reviews, allowing out model to learn the meaning of a review instead of relying on word 
counts. 
Link: https://huggingface.co/sentence-transformers/all-mpnet-base-v2 

Dense Neural Network: 
A sequential dense network will be used to learn the relation between the embeddings 
generated by MPNET and the corresponding score. The input layer will have the same shape 
as the output of MPNET. Two dense layers will be used. The output will be single number 
ranging from 0 to 1, where values closer to 0 reflect negative sentiment and vice-versa. 
Step by Step Implementation 

prepare.py: 
This script is responsible for cleaning the data. It performs the following steps: 
1. Drops the app ID, app name, and review votes columns. 
2. Drop rows with missing values. 
3. Drops duplicate rows. 
4. Replaces -1 in the review score column with 0. This is done because the output of 
Sigmoid function ranges from 0 to 1. 
5. Extracts 10,000 positive and negative samples each. This prevents a class imbalance. 
6. Concatenates and two groups and shuffles them so that positive and negative reviews 
get mixed.  
7. Saves the data as a separate CSV file.

project.ipynb: 
This notebook embeddings for the reviews, trains the model, and evaluates it. 
1. Load the cleaned data. 
2. Initialize an instance of MPNET. 
3. Generate embeddings for the reviews. 
4. Split the data into training and testing groups. 
5. Define the model architecture. 
6. Compile the model. 
7. Start training. Use the early stopper callback to terminate the training when model 
stops improving. 
8. Calculate accuracy and F1 score. 
9. Plot a confusion matrix to get a better idea of the classifications. 
10. Print 10 random misclassified reviews to see where the model is struggling.
    
Performance Analysis 
Accuracy: 86.3% 
The validation accuracy stagnates after a couple of epochs because MPNET has already 
completed the main challenge of learning the semantics of the reviews.  

F1 Score: 0.864 
This indicates that the model is performing equally well for both positive and negative 
reviews. 

Misclassified Reviews: 
Common trends seen in misclassified reviews: 
Mixed review: Contains both praise and criticism. 
Noise: The score does not match the review. 
Model error: The model simply made the wrong prediction despite clear meaning. 
Long reviews: The modelâ€™s performance starts to degrade once the length of the 
reviews get really long. 
Sarcasm: Heavy sarcasm confuses the model. 

It is known that, human annotators tend to agree on sentiment classification 80-85% of the 
time. Therefore, an accuracy of 86.3% is a satisfactory result. Some more powerful 
embeddings models like e5-large-v2 can push the accuracy beyond 90%.  
